{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trajectory_Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1l2Euytd8aS_Pm7jL7AXmeojRxVWUJiEA",
      "authorship_tag": "ABX9TyMqlTRQlgvwY9b+ES0oh/pe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taejin1221/Lab_Experiment/blob/main/Trajectory_Clustering/Trajectory_Clustering_23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U3kbpMkyu-b",
        "outputId": "1669dc89-c2ef-4b10-e29a-cd75a3217d8c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py53_ix80_P7"
      },
      "source": [
        "# Set constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_O8ic8U579R"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VliVYMBYzdTc"
      },
      "source": [
        "import os, cv2, glob\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCbSwbH7ylRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64bbd57-8f41-488a-a4f1-a78e595a1942"
      },
      "source": [
        "WIDTH, HEIGHT = 8, 8\n",
        "\n",
        "EXPERIMENT_DATA = {\n",
        "    'name' : 'Trajectory_Clusetring',\n",
        "    'number' : '23',\n",
        "    'date' : '06-28-2021',\n",
        "    'description' : 'Create Bitmap Encoder\\n'\n",
        "    }\n",
        "\n",
        "ROOT_DIR = os.path.join('/content', 'drive', 'MyDrive', 'University', 'PiLab', 'Experiment')\n",
        "os.chdir(ROOT_DIR)\n",
        "print(os.getcwd())\n",
        "\n",
        "DATA_DIR = os.path.join('/content', 'Bitmap')\n",
        "RESULT_DIR = os.path.join(ROOT_DIR, 'Results')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/University/PiLab/Experiment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmcbMxCg0OEQ"
      },
      "source": [
        "os.chdir(RESULT_DIR)\n",
        "\n",
        "NEW_RESULT_DIR = f\"{EXPERIMENT_DATA['name']}_{EXPERIMENT_DATA['number']}_{EXPERIMENT_DATA['date']}_Data\"\n",
        "if (NEW_RESULT_DIR not in os.listdir()):\n",
        "    os.mkdir(NEW_RESULT_DIR)\n",
        "os.chdir(NEW_RESULT_DIR)\n",
        "\n",
        "with open('Description.txt', 'w') as f:\n",
        "    f.write(EXPERIMENT_DATA['description'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_W6mAzb9dpj",
        "outputId": "07b742a9-764b-425f-fa26-92754bf33f95"
      },
      "source": [
        "os.chdir(DATA_DIR)\n",
        "print(len(os.listdir()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2M8QwAP08h3"
      },
      "source": [
        "# Load and Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKl0Quuk9Gq9"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "os.chdir('/content')\n",
        "zip_file = zipfile.ZipFile(\"Bitmap.zip\")\n",
        "\n",
        "zip_file.extractall()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ChOqx8w07yC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "1711c10c-2bc4-4690-e5b2-732162315626"
      },
      "source": [
        "os.chdir(DATA_DIR)\n",
        "\n",
        "bmp_data = [ ]\n",
        "for name in sorted(glob.glob('*.png')):\n",
        "    bmp_data.append(cv2.imread(name, cv2.IMREAD_GRAYSCALE))\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, sharex = True, sharey = True, figsize = (16, 48))\n",
        "plt.gray()\n",
        "ax[0].imshow(bmp_data[0])\n",
        "ax[1].imshow(bmp_data[1])\n",
        "ax[2].imshow(bmp_data[2])\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x3456 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xawKqhJf1wpz"
      },
      "source": [
        "X_train = np.array(bmp_data).astype('float32') / 255.\n",
        "X_train = np.reshape(X_train, (-1, HEIGHT, WIDTH, 1))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZAbND4T4hYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa0ed936-626f-4fb0-8ee7-8d1593975a86"
      },
      "source": [
        "print(X_train.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18670, 8, 8, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlijqvYs1BVx"
      },
      "source": [
        "# Build Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTFy6_9WrWAN"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to_7r09ztZKr"
      },
      "source": [
        "act, pd = 'relu', 'same'\n",
        "\n",
        "encoder_input = layers.Input((8, 8, 1))\n",
        "x = layers.Conv2D(16, (3, 3), activation = act, padding = pd)(encoder_input)\n",
        "x = layers.MaxPooling2D((2, 2), padding = pd)(x)\n",
        "x = layers.Conv2D(8, (3, 3), activation = act, padding = pd)(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding = pd)(x)\n",
        "x = layers.Conv2D(8, (3, 3), activation = act, padding = pd)(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding = pd)(x)\n",
        "\n",
        "feature_map_shape = x.shape\n",
        "flatten_size = feature_map_shape[1] * feature_map_shape[2] * feature_map_shape[3]\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "encoder_output = layers.Dense(flatten_size , activation = act)(x)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uahr_Agt3GMU",
        "outputId": "818495ff-3759-44e0-d982-d2d8a2325e3f"
      },
      "source": [
        "encoder = keras.Model(encoder_input, encoder_output, name = 'Encoder')\n",
        "encoder.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 8, 8, 1)]         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 8, 8, 16)          160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 4, 4, 8)           1160      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 2, 2, 8)           584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 8)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 8)                 72        \n",
            "=================================================================\n",
            "Total params: 1,976\n",
            "Trainable params: 1,976\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzWPXJ_13MAO"
      },
      "source": [
        "decoder_input = layers.Input((flatten_size))\n",
        "\n",
        "x = layers.Dense(flatten_size, activation = act)(decoder_input)\n",
        "x = layers.Reshape(feature_map_shape[1:])(x)\n",
        "\n",
        "x = layers.Conv2DTranspose(8, (3, 3), activation = act, padding = pd)(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "x = layers.Conv2DTranspose(8, (3, 3), activation = act, padding = pd)(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "x = layers.Conv2DTranspose(16, (3, 3), activation = act, padding = pd)(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "decoder_output = layers.Conv2DTranspose(1, (3, 3), activation = act, padding = pd)(x)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAS1-b7a3vWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a54327-327b-4c56-8bdd-4b626fe0cae6"
      },
      "source": [
        "decoder = keras.Model(decoder_input, decoder_output, name = 'Decoder')\n",
        "decoder.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 8)]               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 1, 1, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 1, 1, 8)           584       \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 2, 2, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 2, 2, 8)           584       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 4, 4, 8)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 4, 4, 16)          1168      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 8, 8, 1)           145       \n",
            "=================================================================\n",
            "Total params: 2,553\n",
            "Trainable params: 2,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFAGru0T33gH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75033930-4754-476f-c6f4-4d58fc6f8ec6"
      },
      "source": [
        "auto_encoder = keras.Model(encoder_input, decoder(encoder(encoder_input)), name = 'Auto_Encoder')\n",
        "auto_encoder.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Auto_Encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 8, 8, 1)]         0         \n",
            "_________________________________________________________________\n",
            "Encoder (Functional)         (None, 8)                 1976      \n",
            "_________________________________________________________________\n",
            "Decoder (Functional)         (None, 8, 8, 1)           2553      \n",
            "=================================================================\n",
            "Total params: 4,529\n",
            "Trainable params: 4,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA04hGvb4llM"
      },
      "source": [
        "auto_encoder.compile('adam', loss = 'mse')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c038FdSo4FpZ"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1qD24Yh4Gmo"
      },
      "source": [
        "EPOCH = 300\n",
        "BATCH = 256"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8GBLQge4H4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f7b4b4-54b5-42d2-ea80-8ef8943154d6"
      },
      "source": [
        "history = auto_encoder.fit(X_train, X_train, epochs = EPOCH, batch_size = BATCH)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "73/73 [==============================] - 32s 7ms/step - loss: 0.2120\n",
            "Epoch 2/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1772\n",
            "Epoch 3/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1518\n",
            "Epoch 4/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1429\n",
            "Epoch 5/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1324\n",
            "Epoch 6/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1279\n",
            "Epoch 7/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1257\n",
            "Epoch 8/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1241\n",
            "Epoch 9/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1227\n",
            "Epoch 10/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1215\n",
            "Epoch 11/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1205\n",
            "Epoch 12/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1195\n",
            "Epoch 13/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1187\n",
            "Epoch 14/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1178\n",
            "Epoch 15/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1172\n",
            "Epoch 16/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1164\n",
            "Epoch 17/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1156\n",
            "Epoch 18/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1144\n",
            "Epoch 19/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1127\n",
            "Epoch 20/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1113\n",
            "Epoch 21/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1104\n",
            "Epoch 22/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1095\n",
            "Epoch 23/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1087\n",
            "Epoch 24/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1081\n",
            "Epoch 25/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1077\n",
            "Epoch 26/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1072\n",
            "Epoch 27/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1066\n",
            "Epoch 28/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1063\n",
            "Epoch 29/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1058\n",
            "Epoch 30/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1056\n",
            "Epoch 31/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1052\n",
            "Epoch 32/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1049\n",
            "Epoch 33/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1049\n",
            "Epoch 34/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1045\n",
            "Epoch 35/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1042\n",
            "Epoch 36/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1038\n",
            "Epoch 37/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1038\n",
            "Epoch 38/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1034\n",
            "Epoch 39/300\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.1032\n",
            "Epoch 40/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1030\n",
            "Epoch 41/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1028\n",
            "Epoch 42/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1027\n",
            "Epoch 43/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1025\n",
            "Epoch 44/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1024\n",
            "Epoch 45/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1022\n",
            "Epoch 46/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1019\n",
            "Epoch 47/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1019\n",
            "Epoch 48/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1018\n",
            "Epoch 49/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1016\n",
            "Epoch 50/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1014\n",
            "Epoch 51/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1013\n",
            "Epoch 52/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1011\n",
            "Epoch 53/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1010\n",
            "Epoch 54/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1008\n",
            "Epoch 55/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1008\n",
            "Epoch 56/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1006\n",
            "Epoch 57/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1005\n",
            "Epoch 58/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1004\n",
            "Epoch 59/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1002\n",
            "Epoch 60/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1002\n",
            "Epoch 61/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.1002\n",
            "Epoch 62/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0999\n",
            "Epoch 63/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0998\n",
            "Epoch 64/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0996\n",
            "Epoch 65/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0996\n",
            "Epoch 66/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0995\n",
            "Epoch 67/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0994\n",
            "Epoch 68/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0994\n",
            "Epoch 69/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0993\n",
            "Epoch 70/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0992\n",
            "Epoch 71/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0991\n",
            "Epoch 72/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0990\n",
            "Epoch 73/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0990\n",
            "Epoch 74/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0990\n",
            "Epoch 75/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0987\n",
            "Epoch 76/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0985\n",
            "Epoch 77/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0985\n",
            "Epoch 78/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0986\n",
            "Epoch 79/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0984\n",
            "Epoch 80/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0983\n",
            "Epoch 81/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0981\n",
            "Epoch 82/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0981\n",
            "Epoch 83/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0982\n",
            "Epoch 84/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0981\n",
            "Epoch 85/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0981\n",
            "Epoch 86/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0980\n",
            "Epoch 87/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0979\n",
            "Epoch 88/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0980\n",
            "Epoch 89/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0976\n",
            "Epoch 90/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0976\n",
            "Epoch 91/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0975\n",
            "Epoch 92/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0975\n",
            "Epoch 93/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0974\n",
            "Epoch 94/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0974\n",
            "Epoch 95/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0974\n",
            "Epoch 96/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0972\n",
            "Epoch 97/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0971\n",
            "Epoch 98/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0971\n",
            "Epoch 99/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0970\n",
            "Epoch 100/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0970\n",
            "Epoch 101/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0969\n",
            "Epoch 102/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0969\n",
            "Epoch 103/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0969\n",
            "Epoch 104/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0968\n",
            "Epoch 105/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0968\n",
            "Epoch 106/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0967\n",
            "Epoch 107/300\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.0965\n",
            "Epoch 108/300\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.0966\n",
            "Epoch 109/300\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.0966\n",
            "Epoch 110/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0964\n",
            "Epoch 111/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0963\n",
            "Epoch 112/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0963\n",
            "Epoch 113/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0964\n",
            "Epoch 114/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0962\n",
            "Epoch 115/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0962\n",
            "Epoch 116/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0962\n",
            "Epoch 117/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0959\n",
            "Epoch 118/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0960\n",
            "Epoch 119/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0961\n",
            "Epoch 120/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0960\n",
            "Epoch 121/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0959\n",
            "Epoch 122/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0958\n",
            "Epoch 123/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0959\n",
            "Epoch 124/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0957\n",
            "Epoch 125/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0957\n",
            "Epoch 126/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0956\n",
            "Epoch 127/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0955\n",
            "Epoch 128/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0958\n",
            "Epoch 129/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0955\n",
            "Epoch 130/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0953\n",
            "Epoch 131/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0954\n",
            "Epoch 132/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0954\n",
            "Epoch 133/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0953\n",
            "Epoch 134/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0953\n",
            "Epoch 135/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0951\n",
            "Epoch 136/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0952\n",
            "Epoch 137/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0953\n",
            "Epoch 138/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0952\n",
            "Epoch 139/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0952\n",
            "Epoch 140/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0949\n",
            "Epoch 141/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0949\n",
            "Epoch 142/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0949\n",
            "Epoch 143/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0950\n",
            "Epoch 144/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0948\n",
            "Epoch 145/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0948\n",
            "Epoch 146/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0948\n",
            "Epoch 147/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0948\n",
            "Epoch 148/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0947\n",
            "Epoch 149/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0948\n",
            "Epoch 150/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0948\n",
            "Epoch 151/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0946\n",
            "Epoch 152/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0945\n",
            "Epoch 153/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0945\n",
            "Epoch 154/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0945\n",
            "Epoch 155/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0946\n",
            "Epoch 156/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0943\n",
            "Epoch 157/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0944\n",
            "Epoch 158/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0944\n",
            "Epoch 159/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0943\n",
            "Epoch 160/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0943\n",
            "Epoch 161/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0944\n",
            "Epoch 162/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0942\n",
            "Epoch 163/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0943\n",
            "Epoch 164/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0943\n",
            "Epoch 165/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0941\n",
            "Epoch 166/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0941\n",
            "Epoch 167/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0942\n",
            "Epoch 168/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0941\n",
            "Epoch 169/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0940\n",
            "Epoch 170/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0941\n",
            "Epoch 171/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0940\n",
            "Epoch 172/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0943\n",
            "Epoch 173/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0941\n",
            "Epoch 174/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0940\n",
            "Epoch 175/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0938\n",
            "Epoch 176/300\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.0939\n",
            "Epoch 177/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0938\n",
            "Epoch 178/300\n",
            "73/73 [==============================] - 0s 5ms/step - loss: 0.0938\n",
            "Epoch 179/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0938\n",
            "Epoch 180/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0937\n",
            "Epoch 181/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0937\n",
            "Epoch 182/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0937\n",
            "Epoch 183/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0937\n",
            "Epoch 184/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0937\n",
            "Epoch 185/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0937\n",
            "Epoch 186/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0935\n",
            "Epoch 187/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0937\n",
            "Epoch 188/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0936\n",
            "Epoch 189/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0935\n",
            "Epoch 190/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0936\n",
            "Epoch 191/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0936\n",
            "Epoch 192/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0934\n",
            "Epoch 193/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0935\n",
            "Epoch 194/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0933\n",
            "Epoch 195/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0935\n",
            "Epoch 196/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0934\n",
            "Epoch 197/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0934\n",
            "Epoch 198/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0935\n",
            "Epoch 199/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0933\n",
            "Epoch 200/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0934\n",
            "Epoch 201/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0931\n",
            "Epoch 202/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0932\n",
            "Epoch 203/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0932\n",
            "Epoch 204/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0933\n",
            "Epoch 205/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0931\n",
            "Epoch 206/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0930\n",
            "Epoch 207/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0930\n",
            "Epoch 208/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0930\n",
            "Epoch 209/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0930\n",
            "Epoch 210/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0928\n",
            "Epoch 211/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0928\n",
            "Epoch 212/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0928\n",
            "Epoch 213/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0927\n",
            "Epoch 214/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0927\n",
            "Epoch 215/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0927\n",
            "Epoch 216/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0927\n",
            "Epoch 217/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0925\n",
            "Epoch 218/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0926\n",
            "Epoch 219/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0927\n",
            "Epoch 220/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0924\n",
            "Epoch 221/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0923\n",
            "Epoch 222/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0923\n",
            "Epoch 223/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0922\n",
            "Epoch 224/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0923\n",
            "Epoch 225/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0921\n",
            "Epoch 226/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0921\n",
            "Epoch 227/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0920\n",
            "Epoch 228/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0918\n",
            "Epoch 229/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0918\n",
            "Epoch 230/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0917\n",
            "Epoch 231/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0919\n",
            "Epoch 232/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0915\n",
            "Epoch 233/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0916\n",
            "Epoch 234/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0915\n",
            "Epoch 235/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0915\n",
            "Epoch 236/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0913\n",
            "Epoch 237/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0913\n",
            "Epoch 238/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0912\n",
            "Epoch 239/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0911\n",
            "Epoch 240/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0911\n",
            "Epoch 241/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0912\n",
            "Epoch 242/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0911\n",
            "Epoch 243/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0910\n",
            "Epoch 244/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0911\n",
            "Epoch 245/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0909\n",
            "Epoch 246/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0908\n",
            "Epoch 247/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0910\n",
            "Epoch 248/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0908\n",
            "Epoch 249/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0906\n",
            "Epoch 250/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0907\n",
            "Epoch 251/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0906\n",
            "Epoch 252/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0905\n",
            "Epoch 253/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0906\n",
            "Epoch 254/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0903\n",
            "Epoch 255/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0904\n",
            "Epoch 256/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0902\n",
            "Epoch 257/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0901\n",
            "Epoch 258/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0900\n",
            "Epoch 259/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0899\n",
            "Epoch 260/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0900\n",
            "Epoch 261/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0899\n",
            "Epoch 262/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0899\n",
            "Epoch 263/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0899\n",
            "Epoch 264/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0897\n",
            "Epoch 265/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0896\n",
            "Epoch 266/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0897\n",
            "Epoch 267/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0893\n",
            "Epoch 268/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0894\n",
            "Epoch 269/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0893\n",
            "Epoch 270/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0892\n",
            "Epoch 271/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0894\n",
            "Epoch 272/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0891\n",
            "Epoch 273/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0891\n",
            "Epoch 274/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0891\n",
            "Epoch 275/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0893\n",
            "Epoch 276/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0890\n",
            "Epoch 277/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0889\n",
            "Epoch 278/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0887\n",
            "Epoch 279/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0887\n",
            "Epoch 280/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0886\n",
            "Epoch 281/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0886\n",
            "Epoch 282/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0886\n",
            "Epoch 283/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0885\n",
            "Epoch 284/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0885\n",
            "Epoch 285/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0884\n",
            "Epoch 286/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0883\n",
            "Epoch 287/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0883\n",
            "Epoch 288/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0882\n",
            "Epoch 289/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0882\n",
            "Epoch 290/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0881\n",
            "Epoch 291/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0881\n",
            "Epoch 292/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0881\n",
            "Epoch 293/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0880\n",
            "Epoch 294/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0880\n",
            "Epoch 295/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0879\n",
            "Epoch 296/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0878\n",
            "Epoch 297/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0878\n",
            "Epoch 298/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0878\n",
            "Epoch 299/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0877\n",
            "Epoch 300/300\n",
            "73/73 [==============================] - 0s 4ms/step - loss: 0.0876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJWHaFaT4NtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d410e9-2a7f-435e-8029-72591c39d76d"
      },
      "source": [
        "os.chdir(os.path.join(RESULT_DIR, NEW_RESULT_DIR))\n",
        "\n",
        "encoder.save('encoder.h5')\n",
        "decoder.save('decoder.h5')\n",
        "auto_encoder.save('auto_encoder.h5')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hn5poVbAwHf"
      },
      "source": [
        "# Test Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y3X-OhmAvRC"
      },
      "source": [
        "low_dimension_data = encoder(X_train)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdbRA4hLA5fs",
        "outputId": "1eefc959-bf4b-4075-a110-de684425946c"
      },
      "source": [
        "low_dimension_data[0]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
              "array([2.2357178, 5.2542934, 4.2102385, 5.590271 , 2.7670023, 0.       ,\n",
              "       1.3716769, 3.4490964], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLJGOl1_A7_Y",
        "outputId": "32c8965d-2666-4fde-e2de-fc9fc6280b09"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "print(distance.cosine(low_dimension_data[0], low_dimension_data[1]))\n",
        "print(distance.cosine(low_dimension_data[0], low_dimension_data[2]))\n",
        "print(distance.cosine(low_dimension_data[1], low_dimension_data[2]))\n",
        "print(distance.cosine(low_dimension_data[2], low_dimension_data[1]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.06379938125610352\n",
            "0.05544102191925049\n",
            "0.06279927492141724\n",
            "0.06279927492141724\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}